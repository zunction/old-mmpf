{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation of the gradient\n",
    "\n",
    "From the earlier discussion, we have the cost function of MPF to be\n",
    "\n",
    "$$ K(\\theta) \\approx \\frac{1}{|\\mathcal{D}|}\\sum_{y\\in \\mathcal{D}}\\sum_{h=1}^{n}\\exp\\bigg\\{\\delta_h * (\\theta y)_h - 1/4 * diag(\\theta)_h\\bigg\\}$$\n",
    "\n",
    "and the gradient of the cost function with respect to the $\\theta$ matrix is\n",
    "\n",
    "$$ \\frac{\\partial K(\\theta)}{\\partial \\theta_{ij}} = \\begin{cases}\\delta_iy_jk_i+\\delta_jy_ik_j & i \\neq j\\\\ \\left(\\delta_iy_i-\\frac{1}{4}\\right)k_i & i = j\\\\ \\end{cases}$$\n",
    "\n",
    "where $k_h = \\exp\\bigg\\{\\delta_h * (\\theta y)_h - 1/4 * diag(\\theta)_h\\bigg\\}$. We shall now work out how to explicitly compute the gradients using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by recalling some definitions:\n",
    "- $s$ :  samples where each row is the number of samples and each columns represent a unit in the restricted boltzmann machine, say $n$.\n",
    "- $\\theta$ : the parameter matrix to be learnt which has a size of $n \\times n$\n",
    "\n",
    "With the energy matrix $E$, we can compute the $\\delta_ik_i$ terms by $\\delta * k$, following by we can obtain the $\\delta_iy_jk_i$ terms by taking the dot product of $\\delta * k$ tranpose and $s$, which we shall call this matrix $D'$ that looks like \n",
    "\n",
    "$$ D'_{ij} = \\begin{cases}\\delta_iy_jk_i & i \\neq j\\\\ \\delta_iy_ik_i & i = j\\\\ \\end{cases}$$\n",
    "\n",
    "we extract the diagonals as $C$ and we add the missing $0.25 * k_i$ term to it by subtracting 0.25 times of the sum of the rows of $k$ from $C$. To form the $d_iy_jk_i + \\delta_jy_ik_j$ term we remove the diagonals of $D'$ and call it $D''$, following which added the transpose of $D''$ to itself. We get the desired gradient matrix by filling the empty diagonals of $D'' + D''^\\top$ with $C$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import theano\n",
    "# import theano.tensor as T\n",
    "\n",
    "\n",
    "import os\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "from mpfntutils import load_data\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unravelparam(theta, units = 16):\n",
    "    \"\"\"\n",
    "    Restores a vector of parameters into matrix form.\n",
    "    \"\"\"\n",
    "    W = theta.reshape(units, units)\n",
    "    return W\n",
    "\n",
    "\n",
    "def ravelparam(W):\n",
    "    \"\"\"\n",
    "    Ravels the parameters into a vector.\n",
    "    \"\"\"\n",
    "    theta = W.ravel()\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "units = 16\n",
    "U = np.random.normal(loc = 0, scale = 1/units, size = (units, units))\n",
    "W = 0.5 * (U + U.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = load_data('16-50K.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Kcost(x, W, temperature = 1):\n",
    "        \"\"\"\n",
    "        Returns the cost computed by using the diagonals as the bias.\n",
    "        Inputs:\n",
    "        - x: samples used to train W.\n",
    "        - W: weights between the neurons of the Boltzmann Machine (BM).\n",
    "        - n: number of neurons in the BM.\n",
    "        - temperature: keep it as 1 until cost grows too big then raise temperature.\n",
    "        \"\"\"\n",
    "        num_samples = x.shape[0]        \n",
    "        num_units = x.shape[1]\n",
    "        delta = 1/2 - x\n",
    "        diag = np.diag(W)[:, None].T\n",
    "        E = delta * np.dot(x, W) - .25 * diag\n",
    "        \n",
    "        cost = np.sum(np.exp(1/temperature * E)) / num_samples         \n",
    "        k = np.exp(E)        \n",
    "        D = np.dot((delta * k).T, x)         \n",
    "        C = np.zeros((num_units,))         \n",
    "        np.copyto(C, np.diag(D))                 \n",
    "        np.fill_diagonal(D, 0)         \n",
    "        C = C - .25 * np.sum(k, axis = 0)         \n",
    "        D = D + D.T         \n",
    "        np.fill_diagonal(D, C) \n",
    "\n",
    "        return cost, D/ num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost, Wgrad = Kcost(samples, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation of Numerical gradient\n",
    "For the computation of the numerical gradient for sanity checking, it is not as straightword as our $W$ matrix here is diagonal, meaning for a $3 \\times 3$ $W$ matrix toy example,\n",
    "\n",
    "$$\\begin{pmatrix}\\theta_{11} & \\theta_{12} & \\theta_{13} \\\\ \\theta_{21} & \\theta_{22} & \\theta_{23} \\\\\\theta_{31} & \\theta_{32} & \\theta_{33}  \\end{pmatrix}$$\n",
    "\n",
    "the gradients of $\\theta_{ij}$ and $\\theta_{ji}$ are though of as the same variables and hence in the computation of the numerical gradient for the parameter $\\theta_{ij}$ using the formula \n",
    "\n",
    "$$\\frac{K(\\theta+\\epsilon) - K(\\theta-\\epsilon)}{2\\epsilon}$$\n",
    "\n",
    "we have to add epsilon to both $\\theta_{ij}$ and $\\theta_{ji}$ to get their gradients (which are the same value). For example, to find the numerical gradient of $\\theta_{12}$ we add epsilon to both $\\theta_{12}$ and $\\theta_{21}$, i.e.\n",
    "\n",
    "$$\\theta + \\epsilon_{12} = \\begin{pmatrix}\\theta_{11} & \\theta_{12} + \\epsilon & \\theta_{13} \\\\ \\theta_{21}+ \\epsilon & \\theta_{22} & \\theta_{23} \\\\\\theta_{31} & \\theta_{32} & \\theta_{33}  \\end{pmatrix}$$\n",
    "\n",
    "here $\\epsilon_{ij}$ is used to denote a small increment to the $\\theta_{ij}$ and $\\theta_{ji}$ value, thus $\\theta + \\epsilon_{ij} = \\theta + \\epsilon_{ji}$. Therefore,\n",
    "\n",
    "$$\\frac{\\partial K(\\theta)}{\\partial\\theta_{ij}} = \\frac{K(\\theta+\\epsilon_{ij}) - K(\\theta-\\epsilon_{ij})}{2\\epsilon}$$\n",
    "\n",
    "To get the $\\theta + \\epsilon_{ij}$ matrix, using transponse, raveling and unraveling of the parameters will do the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeNumericalGradient(J,W):\n",
    "    EPSILON = 0.0001    \n",
    "    W = ravelparam(W) \n",
    "    numgrad = np.zeros(np.shape(W)) \n",
    "    num_para = W.shape[0] \n",
    "    \n",
    "    for i in range(num_para):\n",
    "        w = np.zeros(np.shape(W))\n",
    "        w[i] = EPSILON\n",
    "        e_W = unravelparam(w)\n",
    "        if not (e_W == e_W.T).all():\n",
    "            e_W = ravelparam(e_W + e_W.T)\n",
    "        else:\n",
    "            e_W = ravelparam(e_W)\n",
    "        wp = unravelparam(W + e_W)\n",
    "        wm = unravelparam(W - e_W)\n",
    "        numgrad[i] = (J(wp) - J(wm)) / (2 * EPSILON)\n",
    "       \n",
    "    return unravelparam(numgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numgrad = computeNumericalGradient(lambda x: Kcost(samples, x)[0], W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing of analytic and numerical gradient\n",
    "After getting both your numerical gradient, we error difference should be on the order of $1^{-10}$, which means that you analytic gradient that you implemented above is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.60852798632e-10\n"
     ]
    }
   ],
   "source": [
    "diff = norm(numgrad-Wgrad)/norm(numgrad+Wgrad)\n",
    "print (diff)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
