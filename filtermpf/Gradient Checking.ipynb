{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of the gradient\n",
    "\n",
    "From the earlier discussion, we have the cost function of MPF to be\n",
    "\n",
    "$$ K(\\theta) \\approx \\frac{1}{|\\mathcal{D}|}\\sum_{y\\in \\mathcal{D}}\\sum_{h=1}^{n}\\exp\\bigg\\{\\delta_h * (Wy)_h - 1/4 * diag(W)_h\\bigg\\}$$\n",
    "\n",
    "and the gradient of the cost function with respect to the $W$ matrix is\n",
    "\n",
    "$$ \\frac{\\partial K(\\theta)}{\\partial W_{ij}} = \\begin{cases}\\delta_iy_jk_i+\\delta_jy_ik_j & i \\neq j\\\\ \\left(\\delta_iy_i-\\frac{1}{4}\\right)k_i & i = j\\\\ \\end{cases}$$\n",
    "\n",
    "where $k_h = \\exp\\bigg\\{\\delta_h * (Wy)_h - 1/4 * diag(W)_h\\bigg\\}$. We shall now work out how to explicitly compute the gradients using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by recalling some definitions:\n",
    "- $s$ : samples where each row is the number of samples and each columns represent a unit in the restricted boltzmann machine, say $n$.\n",
    "- $W$: the parameter matrix to be learnt which has a size of $n \\times n$\n",
    "\n",
    "With the energy matrix $E$, we can compute the $\\delta_ik_i$ terms by $\\delta * k$, following by we can obtain the $\\delta_iy_jk_i$ terms by taking the dot product of $\\delta * k$ tranpose and $s$, which we shall call this matrix $D'$ that looks like \n",
    "\n",
    "$$ D'_{ij} = \\begin{cases}\\delta_iy_jk_i & i \\neq j\\\\ \\delta_iy_ik_i & i = j\\\\ \\end{cases}$$\n",
    "\n",
    "we extract the diagonals as $C$ and we add the missing $0.25 * k_i$ term to it by subtracting 0.25 times of the sum of the rows of $k$ from $C$. To form the $d_iy_jk_i + \\delta_jy_ik_j$ term we remove the diagonals of $D'$ and call it $D''$, following which added the transpose of $D''$ to itself. We get the desired gradient matrix by filling the empty diagonals of $D'' + D''^\\top$ with $C$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import theano\n",
    "# import theano.tensor as T\n",
    "\n",
    "\n",
    "import os\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "from mpfntutils import load_data\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unravelparam(theta, units = 16):\n",
    "    \"\"\"\n",
    "    Restores a vector of parameters into matrix form.\n",
    "    \"\"\"\n",
    "    W = theta.reshape(units, units)\n",
    "    return W\n",
    "\n",
    "\n",
    "def ravelparam(W):\n",
    "    \"\"\"\n",
    "    Ravels the parameters into a vector.\n",
    "    \"\"\"\n",
    "    theta = W.ravel()\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "units = 16\n",
    "U = np.random.normal(loc = 0, scale = 1/units, size = (units, units))\n",
    "W = 0.5 * (U + U.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = load_data('16-50K.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Kcost(x, W, temperature = 1):\n",
    "        \"\"\"\n",
    "        Returns the cost computed by using the diagonals as the bias.\n",
    "        Inputs:\n",
    "        - x: samples used to train W.\n",
    "        - W: weights between the neurons of the Boltzmann Machine (BM).\n",
    "        - n: number of neurons in the BM.\n",
    "        - temperature: keep it as 1 until cost grows too big then raise temperature.\n",
    "        \"\"\"\n",
    "        num_samples = x.shape[0]        \n",
    "        num_units = x.shape[1]\n",
    "        delta = 1/2 - x\n",
    "        diag = np.diag(W)[:, None].T\n",
    "        E = delta * np.dot(x, W) - .25 * diag\n",
    "        \n",
    "        cost = np.sum(np.exp(1/temperature * E)) / num_samples # the cost should be correct here\n",
    "        \n",
    "        # computation of gradient\n",
    "        \n",
    "        k = np.exp(E)\n",
    "        \n",
    "        D = np.dot((delta * k).T, x) # delta_i y_j k_i term from dot product of delta * k and samples\n",
    "        \n",
    "        C = np.zeros((num_units,)) # initialize diagonal gradients\n",
    "        \n",
    "        np.copyto(C, np.diag(D)) # extract out delta_i y_i k_i terms for later use\n",
    "                \n",
    "        np.fill_diagonal(D, 0) # set diagonals of D to zeros\n",
    "        \n",
    "        C = C - .25 * np.sum(k, axis = 0) # evaluation of (delta_i y_i - .25) k_i\n",
    "        \n",
    "        D = D + D.T # forming of delta_i y_j k_i + delta_j y_i k_j terms for i not equals to j\n",
    "        \n",
    "        np.fill_diagonal(D, C) # fill in the diagonals back to the gradient matrix\n",
    "\n",
    "        return cost, D/ num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidied version of Kcost with should be correct cost and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Kcost(x, W, temperature = 1):\n",
    "        \"\"\"\n",
    "        Returns the cost computed by using the diagonals as the bias.\n",
    "        Inputs:\n",
    "        - x: samples used to train W.\n",
    "        - W: weights between the neurons of the Boltzmann Machine (BM).\n",
    "        - n: number of neurons in the BM.\n",
    "        - temperature: keep it as 1 until cost grows too big then raise temperature.\n",
    "        \"\"\"\n",
    "        num_samples = x.shape[0]        \n",
    "        num_units = x.shape[1]\n",
    "        delta = 1/2 - x\n",
    "        diag = np.diag(W)[:, None].T\n",
    "        E = delta * np.dot(x, W) - .25 * diag\n",
    "        \n",
    "        cost = np.sum(np.exp(1/temperature * E)) / num_samples         \n",
    "        k = np.exp(E)        \n",
    "        D = np.dot((delta * k).T, x)         \n",
    "        C = np.zeros((num_units,))         \n",
    "        np.copyto(C, np.diag(D))                 \n",
    "        np.fill_diagonal(D, 0)         \n",
    "        C = C - .25 * np.sum(k, axis = 0)         \n",
    "        D = D + D.T         \n",
    "        np.fill_diagonal(D, C) \n",
    "\n",
    "        return cost, D/ num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost, Wgrad = Kcost(samples, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.962510117\n",
      "(16, 16)\n"
     ]
    }
   ],
   "source": [
    "print (cost)\n",
    "print (Wgrad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to find problem with numgrad\n",
    "- need to resolve the dimension problem in 21 (done)\n",
    "- diagonals have correct gradient but the other gradients are wrong, look at 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeNumericalGradient(J,W):\n",
    "    \n",
    "    EPSILON = 0.0001\n",
    "    mat_W_shape = W.shape # find shape of initial matrix\n",
    "    e_mat = EPSILON * np.ones(mat_W_shape) # \n",
    "    \n",
    "    W = ravelparam(W) # W is a n x n matrix, ravel it to a n **2 vector\n",
    "    numgrad = np.zeros(np.shape(W)) # numgrad will have a n **2 dimensional vector\n",
    "\n",
    "    \n",
    "\n",
    "    num_para = W.shape[0] \n",
    "    \n",
    "    \n",
    "    e_plus = EPSILON * np.eye(num_para) # n **2 by n **2 matrix with diagonals EPSILON and 0 otherwise\n",
    "    e_minus = - e_plus # negation of e_plus\n",
    "    theta_e_plus = e_plus + W \n",
    "    theta_e_minus = e_minus + W\n",
    "    \n",
    "    # loop over number of parameters to compute the gradient for each paramter\n",
    "    for i in range(num_para):\n",
    "        p = unravelparam(theta_e_plus[i, :])\n",
    "        m = unravelparam(theta_e_minus[i, :])\n",
    "        numgrad[i] = (J(p) - J(m))/ (2 * EPSILON)\n",
    "        \n",
    "    return unravelparam(numgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numgrad = computeNumericalGradient(lambda x: Kcost(samples, x)[0], W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16)\n"
     ]
    }
   ],
   "source": [
    "print (numgrad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.344519137321\n"
     ]
    }
   ],
   "source": [
    "diff = norm(numgrad-Wgrad)/norm(numgrad+Wgrad)\n",
    "print (diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.6736862  -0.58469385 -0.41706637  0.16497182  0.10624317 -0.78368   ]\n",
      " [-0.58469385 -0.61328071 -0.34259382  0.10182234  0.08588656 -0.64657365]]\n",
      "[[-0.6736862  -0.26373654 -0.17178938  0.24298219  0.20236892 -0.42030855]\n",
      " [-0.32095731 -0.61328071 -0.16204418  0.19074741  0.17205835 -0.38150326]]\n"
     ]
    }
   ],
   "source": [
    "print (Wgrad[0:2,:6])\n",
    "print (numgrad[0:2,:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wgrad[2,3] == Wgrad[3,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24404546683953754"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numgrad[2,3] - numgrad[3,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
