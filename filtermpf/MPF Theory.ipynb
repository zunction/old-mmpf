{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective function for Minimum Probability Flow\n",
    "Given some observations $\\mathcal{D}$, we can estimate the parameter using maximum likelihood which is also equivalent to minimizing the KL divergence between the model distribution $\\mathbf{p}^{(0)}$ and the model distribution $\\mathbf{p}^{(\\infty)}$\n",
    "\n",
    "$$\\hat{\\theta}_{ML} = argmin_{\\theta}~ D_{KL}\\left(\\mathbf{p}^{(0)}||\\mathbf{p}^{(\\infty)}(\\theta)\\right)$$\n",
    "\n",
    "In MPF, instead of running the dynamics for infinite time, we minimize the KL divergence after running the dynamics for infinitesimal time $\\epsilon$,\n",
    "\n",
    "$$\\hat{\\theta}_{MPF} = argmin_{\\theta}~K(\\theta) $$\n",
    "where $$K(\\theta) = D_{KL}\\left(\\mathbf{p}^{(0)}||\\mathbf{p}^{(\\epsilon)}(\\theta)\\right)$$\n",
    "\n",
    "Since $\\epsilon$ is taken to be small, we can perform first order [Taylor expansion](https://en.wikipedia.org/wiki/Taylor_series),\n",
    "\n",
    "$$K(\\theta) \\approx D_{KL}\\left(\\mathbf{p}^{(0)}||\\mathbf{p}^{(t)}(\\theta)\\right)\\Big|_{t=0} + \\epsilon \\frac{\\partial D_{KL}\\left(\\mathbf{p}^{(0)}||\\mathbf{p}^{(t)}(\\theta)\\right)}{\\partial t}\\Bigg|_{t=0}$$\n",
    "\n",
    "The first term is zero so we evaluate the derivative in the second term,\n",
    "\n",
    "$$\\frac{\\partial D_{KL}\\left(\\mathbf{p}^{(0)}||\\mathbf{p}^{(t)}(\\theta)\\right)}{\\partial t}\\Bigg|_{t=0} = -\\sum_{x \\in \\mathcal{D}}p^{(0)}(x)\\frac{\\partial}{\\partial t}\\log p^{(t)}(x|\\theta)\\Bigg|_{t=0}$$\n",
    "\n",
    "$$ = -\\sum_{x \\in \\mathcal{D}}p^{(0)}(x)\\frac{\\dot{p}^{(t)}(x|\\theta)}{p^{(t)}(x|\\theta)}\\Bigg|_{t=0}$$\n",
    "\n",
    "$$ = -\\sum_{x \\in \\mathcal{D}}\\dot{p}^{(0)}(x|\\theta)$$\n",
    "\n",
    "$$ = -\\sum_{x \\in \\mathcal{D}}\\left(\\sum_{\\substack{y \\in \\mathcal{X}\\\\y \\neq x}}\\Gamma_{xy}\\,p^{(0)}(y)-\\sum_{\\substack{y \\in \\mathcal{X}\\\\y \\neq x}}\\Gamma_{yx}\\,p^{(0)}(x)\\right)$$\n",
    "\n",
    "## Still not completed as I don't understand some parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boltzmann Machine network\n",
    "We consider the Boltzmann Machine where the connectivity of the different states is given to be $g_{xy} \\neq 0$ if and only if $x$ and $y$ differ only at exactly one bit location. We also think of $W$ to be **symmetric** with the diagonals thought of as the biases. Previously, we set the diagonals to be zeros with the biases computed separately. With this new implementation, the parameters and biases are stored in a single matrix $W$.\n",
    "\n",
    "Suppose that $x$ differs from $y$ at the $h$ location, and let $y_{-h}$ denote the elements of $y$ except the one at the $h$ location,\n",
    "\n",
    "$$E(y|\\theta) - E(x|\\theta) = E(y_h,y_{-h}|\\theta) - E(1-y_h,y_{-h}|\\theta)$$\n",
    "\n",
    "$$=-\\frac{1}{2}y^\\top \\theta y + \\frac{1}{2}x^\\top \\theta x$$\n",
    "\n",
    "we can do a simple expansion which gives \n",
    "\n",
    "$$ \\frac{1}{2}(x^\\top \\theta x -x^\\top \\theta y + x^\\top \\theta y - y^\\top \\theta y )$$\n",
    "\n",
    "after some simplification we get\n",
    "\n",
    "$$\\frac{1}{2}a^\\top \\theta b$$\n",
    "\n",
    "where $a = x + y$ and $b = x -y$. Writing out $x$ and $y$ explicitly,\n",
    "\n",
    "$$y=\\begin{pmatrix}y_1\\\\ \\vdots\\\\ y_h\\\\ \\vdots\\\\ y_n\\end{pmatrix} \\quad x = \\begin{pmatrix}y_1\\\\ \\vdots\\\\1-y_h\\\\ \\vdots\\\\ y_n\\end{pmatrix}$$\n",
    "\n",
    "Solving for the $a$ and $b$, we have\n",
    "\n",
    "$$a=\\begin{pmatrix}2y_1\\\\ \\vdots\\\\ 1\\\\ \\vdots\\\\ 2y_n\\end{pmatrix} \\quad b = \\begin{pmatrix}0\\\\ \\vdots\\\\1-2y_h\\\\ \\vdots\\\\ 0\\end{pmatrix}$$\n",
    "\n",
    "thus\n",
    "\n",
    "$$\\frac{1}{2}E(y|\\theta) - E(x|\\theta) = \\frac{1}{4}a^\\top \\theta b = \\begin{pmatrix}y_1(1/2-y_h)\\theta_{1h}, \\ldots,1/2(1/2-y_h)\\theta_{hh}\\ldots,y_n(1/2-x_h)\\theta_{nh} \\end{pmatrix}$$\n",
    "\n",
    "We can then factorize the common $1/2 - y_h$ term out and letting $\\delta = 1/2 - y$, with $*$ denoting element-wise product, we have\n",
    "\n",
    "$$(1/2-y_h)*((\\theta y)_h - \\theta_{hh}(1/2-y_h))$$\n",
    "\n",
    "where subscript $h$ means for row $h$. So finally we derived the following expression,\n",
    "\n",
    "$$\\frac{1}{2}(E(y|\\theta)-E(x|\\theta)) = \\delta_h * (\\theta y)_h - 1/4 * diag(\\theta )_h$$\n",
    "\n",
    "which says that for any two different states that differs at exactly one bit location, say $h$, we can find the difference of the energy of these two states by looking the row $h$ of \n",
    "\n",
    "$$\\delta * \\theta y - 1/4 * diag(\\theta )$$\n",
    "\n",
    "This is a useful result as for every $y \\in \\mathcal{D}$, we would approximate the states that have a one bit difference to be not in $\\mathcal{D}$ which simplifies the computation of the cost function. Hence the cost contributed for each $y \\in \\mathcal{D}$, \n",
    "\n",
    "$$\\sum_{h=1}^{n}\\exp\\bigg\\{\\delta_h * (\\theta y)_h - 1/4 * diag(\\theta )_h\\bigg\\}$$\n",
    "\n",
    "and we have the cost function of MPF to be\n",
    "\n",
    "$$ K(\\theta) \\approx \\frac{1}{|\\mathcal{D}|}\\sum_{y\\in \\mathcal{D}}\\sum_{h=1}^{n}\\exp\\bigg\\{\\delta_h * (\\theta y)_h - 1/4 * diag(\\theta )_h\\bigg\\}$$\n",
    "\n",
    "The gradient of the cost function with respect to the $\\theta$ matrix is\n",
    "\n",
    "$$ \\frac{\\partial K(\\theta)}{\\partial \\theta_{ij}} = \\begin{cases}\\delta_iy_jk_i+\\delta_jy_ik_j & i \\neq j\\\\ \\left(\\delta_iy_i-\\frac{1}{4}\\right)k_i & i = j\\\\ \\end{cases}$$\n",
    "\n",
    "where $k_h = \\exp\\bigg\\{\\delta_h * (\\theta y)_h - 1/4 * diag(\\theta)_h\\bigg\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things below here can be deleted away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import theano\n",
    "# import theano.tensor as T\n",
    "\n",
    "\n",
    "import os\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "from mpfntutils import load_data\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initalizeParameters(units = 32):\n",
    "    \"\"\"\n",
    "    Initialize an initial symmetric W matrix from a random normal distribution with mean 0 and variance 1/n.\n",
    "    Inputs:\n",
    "    - n: size of the Boltzmann Machine (BM).\n",
    "    \"\"\"\n",
    "    U = np.random.normal(loc = 0, scale = 1/units, size = (units, units))\n",
    "    return 0.5 * (U + U.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# old cost and gradient    \n",
    "\n",
    "# def Kcost(x, W, temperature = 1, units = 32):\n",
    "#         \"\"\"\n",
    "#         Returns the cost computed by using the diagonals as the bias.\n",
    "#         Inputs:\n",
    "#         - x: samples used to train W.\n",
    "#         - W: weights between the neurons of the Boltzmann Machine (BM).\n",
    "#         - n: number of neurons in the BM.\n",
    "#         - temperature: keep it as 1 until cost grows too big then raise temperature.\n",
    "#         \"\"\"\n",
    "        \n",
    "        \n",
    "#         cost = np.mean(np.exp(1/temperature * ((0.5 - x) * np.dot(x, W)) \\\n",
    "#                               - 0.25 * np.diag(W)))\n",
    "               \n",
    "#         D = np.dot((0.5 - x).T, x)\n",
    "#         Wgrad = cost * ((D + D.T) - (D * np.eye(units)) + 0.25 * np.eye(units))\n",
    "#         return cost  \n",
    "#         new one starts after here\n",
    "        \n",
    "def Kcost(x, W, temperature = 1):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the cost computed by using the diagonals as the bias.\n",
    "    Inputs:\n",
    "    - x: samples used to train W.\n",
    "    - W: weights between the neurons of the Boltzmann Machine (BM).\n",
    "    - n: number of neurons in the BM.\n",
    "    - temperature: keep it as 1 until cost grows too big then raise temperature.\n",
    "    \"\"\"\n",
    "    num_samples = x.shape[0]        \n",
    "    num_units = x.shape[1]\n",
    "    delta = 1/2 - x\n",
    "    diag = np.diag(W)[:, None].T\n",
    "    E = delta * np.dot(x, W) - .25 * diag\n",
    "\n",
    "    cost = np.sum(np.exp(1/temperature * E)) / num_samples         \n",
    "    k = np.exp(E)        \n",
    "    D = np.dot((delta * k).T, x)         \n",
    "    C = np.zeros((num_units,))         \n",
    "    np.copyto(C, np.diag(D))                 \n",
    "    np.fill_diagonal(D, 0)         \n",
    "    C = C - .25 * np.sum(k, axis = 0)         \n",
    "    D = D + D.T         \n",
    "    np.fill_diagonal(D, C) \n",
    "\n",
    "    return cost, D/ num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def params_adam_opt(grad, m = 0, v = 0, beta1 = 0.9, beta2 = 0.999):\n",
    "    \"\"\"\n",
    "    Adaptive Moment Estimation (Adam) optimizer. Takes in parameters beta1, beta2 for adaptive \n",
    "    optimization. Returns next iteration of m and v for computing the grad update.\n",
    "    \n",
    "    Reference: http://arxiv.org/abs/1412.6980\n",
    "    \"\"\"\n",
    "    m = beta1 * m + (1 - beta1) * grad\n",
    "    v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "    return m, v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainmpf(units = 32, lr = 1e-2, eps = 1e-8, n_epochs = 1000,\n",
    "             batchsize = 32, temperature = 1, validate_every = 100, sample = '32-50K.npy'):\n",
    "    \"\"\"\n",
    "    Trains parameters using MPF without using symbolic gradient.\n",
    "    Inputs:\n",
    "    - units: number of neurons in the BM.\n",
    "    - lr: learning rate.\n",
    "    - eps: parameter for Adam optimizer\n",
    "    - n_epochs: number of epochs to train.\n",
    "    - batchsize: size of batches for stochastic gradient descent.\n",
    "    - temperature: happy to be 1 for now.\n",
    "    - sample: sample used.\n",
    "    \n",
    "    \"\"\"\n",
    "    print (51 * '=')\n",
    "    print (24 * '#' + 'MPF' + 24 * '#')\n",
    "    print (51 * '=')\n",
    "    print (str(datetime.now()))\n",
    "    print ('Input size: {0}'.format(units))\n",
    "    print ('Learning temperature: {0}'.format(temperature))\n",
    "    print ('Learning rate: {0}'.format(lr))\n",
    "    \n",
    "    dataset = load_data(sample)\n",
    "    n_dataset_batches = dataset.shape[0] // batchsize\n",
    "    W = initalizeParameters(units = units)\n",
    "    \n",
    "    print ('Sample used: {0}'.format(sample))\n",
    "    print ('=' * 51)\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "# #     Extracting learnt parameters\n",
    "#     W_learnt = np.zeros((units, units))\n",
    "#     b_learnt = np.zeros((units,))\n",
    "# #     Renaming to W_learnt and b_learnt\n",
    "#     np.copyto(b_learnt, np.diag(W))\n",
    "#     np.copyto(W_learnt, W)\n",
    "#     np.fill_diagonal(W_learnt, 0)\n",
    "\n",
    "#     Loading the original W and b\n",
    "    org_W = np.load(sample[0:2] + '-' + 'W' + '.npy')\n",
    "    org_b = np.load(sample[0:2] + '-' + 'b' + '.npy')\n",
    "   \n",
    "    \n",
    "#     Tracking best parameters learnt\n",
    "    best_mse = np.inf\n",
    "    best_W = [None, np.inf]\n",
    "    best_b = [None, np.inf]\n",
    "    best_epoch = np.inf\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        c = []\n",
    "        current_time = timeit.default_timer()\n",
    "        for batch_index in range(n_dataset_batches):\n",
    "            minibatch = dataset[batch_index * batchsize: (batch_index + 1) * batchsize]\n",
    "            cost, grad = Kcost(dataset, W)\n",
    "#             grad = Kgrad(minibatch, W)\n",
    "#             Extracting learnt parameters\n",
    "            W_learnt = np.zeros((units, units))\n",
    "            b_learnt = np.zeros((units,))\n",
    "#             Renaming to W_learnt and b_learnt\n",
    "            np.copyto(b_learnt, np.diag(W))\n",
    "            np.copyto(W_learnt, W)\n",
    "            np.fill_diagonal(W_learnt, 0)\n",
    "            \n",
    "            m, v = params_adam_opt(grad)\n",
    "            W += -lr * m / (np.sqrt(v) + eps)\n",
    "            c.append(cost)\n",
    "            \n",
    "\n",
    "        mseW = np.linalg.norm(org_W - W_learnt)/ ((units**2 - units) / 2)\n",
    "        mseb = np.linalg.norm(org_b - b_learnt)/ units\n",
    "        mse = mseW + mseb\n",
    "\n",
    "\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_W[0] = W_learnt  \n",
    "            best_W[1] = mseW\n",
    "            best_b[0] = b_learnt  \n",
    "            best_b[1] = mseb\n",
    "            best_cost = np.mean(c, dtype='float64')\n",
    "            best_epoch = epoch\n",
    "\n",
    "        if epoch%validate_every == 0:\n",
    "            print ('Training epoch %d/%d, Cost: %f mseW: %.5f, mseb: %.5f, mse: %.5f, Time Elasped: %.2f '\\\n",
    "                 % (epoch, n_epochs, np.mean(c, dtype='float64'), \\\n",
    "                 mseW, mseb, mse,  (current_time - start_time)/60) )\n",
    "    \n",
    "    end_time = timeit.default_timer()\n",
    "    \n",
    "    training_time = end_time - start_time\n",
    "    fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize=(20,10))\n",
    "    fig.tight_layout()\n",
    "    plt.setp(ax, xticks=np.arange(0,100,16))\n",
    "    ax[0,0].plot(org_W.reshape(-1,1)[0:100], 'r')\n",
    "#     ax[0,0].plot(W_learnt.reshape(-1,1)[0:100], 'b')\n",
    "    ax[0,0].plot(best_W[0].reshape(-1,1)[0:100], 'g')\n",
    "    ax[0,0].set_title('W')\n",
    "    ax[0,0].legend(['W', 'Learnt W','Best W'])\n",
    "    ax[0,1].plot(org_W.reshape(-1,1)[101:200], 'r')\n",
    "#     ax[0,1].plot(W_learnt.reshape(-1,1)[101:200], 'b')\n",
    "    ax[0,1].plot(best_W[0].reshape(-1,1)[101:200], 'g')\n",
    "    ax[0,1].set_title('W')\n",
    "    ax[0,1].legend(['W', 'Learnt W','Best W'])\n",
    "    ax[1,0].plot(org_W.reshape(-1,1)[201:256], 'r')\n",
    "#     ax[1,0].plot(W_learnt.reshape(-1,1)[201:256], 'b')\n",
    "    ax[1,0].plot(best_W[0].reshape(-1,1)[201:256], 'g')\n",
    "    ax[1,0].set_title('W')\n",
    "    ax[1,0].legend(['W', 'Learnt W','Best W'])\n",
    "    ax[1,1].plot(org_b.reshape(-1,1), 'r')\n",
    "#     ax[1,1].plot(b_learnt.reshape(-1,1),'b')\n",
    "    ax[1,1].plot(best_b[0].reshape(-1,1),'g')\n",
    "    ax[1,1].set_title('b')\n",
    "    ax[1,1].legend(['b', 'Learnt b','Best b'])\n",
    "\n",
    "    \n",
    "    print ('The training took %.2f minutes' % (training_time/60.))\n",
    "    print ('#' * 22 + 'Results' + '#' * 22)\n",
    "    print ('=' * 51)\n",
    "    print ('Best mse: {0}'.format(best_mse))\n",
    "    print ('Best W mse: {0}'.format(best_W[1]))\n",
    "    print ('Best b mse: {0}'.format(best_b[1]))\n",
    "    print ('Best epoch: {0}'.format(best_epoch))\n",
    "    print ('=' * 51)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "########################MPF########################\n",
      "===================================================\n",
      "2017-05-31 20:58:04.769466\n",
      "Input size: 16\n",
      "Learning temperature: 1\n",
      "Learning rate: 0.01\n",
      "Sample used: 16-50K.npy\n",
      "===================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-402a2e40d5eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m trainmpf(units = 16, lr = 1e-2, eps = 1e-8, n_epochs = 10,\n\u001b[0;32m----> 2\u001b[0;31m              batchsize = 16, temperature = 1, validate_every = 100, sample = '16-50K.npy')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-0c77ab1d801d>\u001b[0m in \u001b[0;36mtrainmpf\u001b[0;34m(units, lr, eps, n_epochs, batchsize, temperature, validate_every, sample)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_dataset_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mminibatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_index\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKcost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[1;31m#             grad = Kgrad(minibatch, W)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[1;31m#             Extracting learnt parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-967ae963f7dc>\u001b[0m in \u001b[0;36mKcost\u001b[0;34m(x, W, temperature)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m.25\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdiag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtemperature\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainmpf(units = 16, lr = 1e-2, eps = 1e-8, n_epochs = 1000,\n",
    "             batchsize = 16, temperature = 1, validate_every = 100, sample = '16-50K.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things below here are kept in cases needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flow = mpf(input = s, n = 16, temperature = 1)\n",
    "cost, grad = flow.Kcost(lr = 1e-2)\n",
    "print (cost)\n",
    "print (grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainmpf(units = 16, lr = 1e-2, n_epochs = 1000,\n",
    "             batchsize = 16, temperature = 1, validate_every = 100, sample = '16-50K.npy'):\n",
    "    \"\"\"\n",
    "    Trains parameters using MPF.\n",
    "    \"\"\"\n",
    "    \n",
    "    index = T.lscalar()\n",
    "    x = T.matrix('x')\n",
    "    flow = mpf(input = x, n = units, temperature = temperature)\n",
    "    \n",
    "    cost, updates = flow.cost(lr = lr)\n",
    "    \n",
    "    dataset = load_data(sample)\n",
    "    n_dataset_batches = dataset.get_value(borrow = True).shape[0] // batchsize\n",
    "\n",
    "    print ('Sample used: {0}'.format(sample))\n",
    "    print ('=' * 51)\n",
    "    \n",
    "    mpf_cost = theano.function(inputs = [index], outputs = cost, updates = updates, \\\n",
    "                                givens = {x: dataset[index * batchsize: (index + 1) * batchsize]})\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    best_mse = np.inf\n",
    "    best_W = [None, np.inf]\n",
    "    best_b = [None, np.inf]\n",
    "    best_epoch = np.inf\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        c = []\n",
    "        current_time = timeit.default_timer()\n",
    "        for batch_index in range(n_dataset_batches):\n",
    "            c.append(mpf_cost(batch_index))\n",
    "        \n",
    "        W_learnt = flow.W.get_value(borrow = True)\n",
    "        b_learnt = np.zeros((units,))\n",
    "        np.copyto(b_learnt, np.diag(W_learnt))                        \n",
    "        np.fill_diagonal(W_learnt, 0)\n",
    "        \n",
    "        W = np.load(sample[0:2] + '-' + 'W' + '.npy')\n",
    "        b = np.load(sample[0:2] + '-' + 'b' + '.npy')\n",
    "                            \n",
    "        mseW = np.linalg.norm(W - (W_learnt))/ (units**2)\n",
    "        mseb = np.linalg.norm(b - b_learnt)/ units\n",
    "        mse = mseW + mseb\n",
    "\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_W[0] = W_learnt  \n",
    "            best_W[1] = mseW\n",
    "            best_b[0] = b_learnt  \n",
    "            best_b[1] = mseb\n",
    "            best_cost = np.mean(c, dtype='float64')\n",
    "            best_epoch = epoch\n",
    "        \n",
    "        if epoch%validate_every == 0:\n",
    "            print ('Training epoch %d/%d, Cost: %f mseW: %.5f, mseb: %.5f, mse: %.5f, Time Elasped: %.2f '\\\n",
    "                 % (epoch, n_epochs, np.mean(c, dtype='float64'), \\\n",
    "                 mseW, mseb, mse,  (current_time - start_time)/60) )\n",
    "    \n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    training_time = end_time - start_time\n",
    "    fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize=(20,10))\n",
    "#     fig, ax = plt.subplots(figsize=(20,10))\n",
    "    fig.tight_layout()\n",
    "    plt.setp(ax, xticks=np.arange(0,100,units))\n",
    "\n",
    "    ax[0,0].plot(W.reshape(-1,1)[0:100], 'r')\n",
    "    ax[0,0].plot(W_learnt.reshape(-1,1)[0:100], 'b')\n",
    "    ax[0,0].plot(best_W[0].reshape(-1,1)[0:100], 'g')\n",
    "    ax[0,0].set_title('W')\n",
    "    ax[0,0].legend(['W', 'Learnt W','Best W'])\n",
    "    ax[0,1].plot(W.reshape(-1,1)[101:200], 'r')\n",
    "    ax[0,1].plot(W_learnt.reshape(-1,1)[101:200], 'b')\n",
    "    ax[0,1].plot(best_W[0].reshape(-1,1)[101:200], 'g')\n",
    "    ax[0,1].set_title('W')\n",
    "    ax[0,1].legend(['W', 'Learnt W','Best W'])\n",
    "    ax[1,0].plot(W.reshape(-1,1)[201:256], 'r')\n",
    "    ax[1,0].plot(W_learnt.reshape(-1,1)[201:256], 'b')\n",
    "    ax[1,0].plot(best_W[0].reshape(-1,1)[201:256], 'g')\n",
    "    ax[1,0].set_title('W')\n",
    "    ax[1,0].legend(['W', 'Learnt W','Best W'])\n",
    "    ax[1,1].plot(b.reshape(-1,1), 'r')\n",
    "    ax[1,1].plot(b_learnt.reshape(-1,1),'b')\n",
    "    ax[1,1].plot(best_b[0].reshape(-1,1),'g')\n",
    "    ax[1,1].set_title('b')\n",
    "    ax[1,1].legend(['b', 'Learnt b','Best b'])\n",
    "\n",
    "    \n",
    "    print ('The training took %.2f minutes' % (training_time/60.))\n",
    "    print ('#' * 22 + 'Results' + '#' * 22)\n",
    "    print ('=' * 51)\n",
    "    print ('Best mse: {0}'.format(best_mse))\n",
    "    print ('Best W mse: {0}'.format(best_W[1]))\n",
    "    print ('Best b mse: {0}'.format(best_b[1]))\n",
    "    print ('Best epoch: {0}'.format(best_epoch))\n",
    "    print ('=' * 51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class mpf(object):\n",
    "    \"\"\"\n",
    "    Minimum probability flow with bias in the diagonals\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input, n = 32, temperature = 1,  W = None):\n",
    "        \n",
    "        self.n = n\n",
    "        self.temperature = temperature\n",
    "        U = np.random.normal(loc = 0, scale = 1/self.n, size = (self.n, self.n))\n",
    "\n",
    "        if not W:\n",
    "            W = 0.5 * (U + U.T)\n",
    "\n",
    "        self.W = W\n",
    "        self.x = input\n",
    "        \n",
    "    def Kcost(self, lr = 1e-2):\n",
    "        \"\"\"\n",
    "        Returns the cost. \n",
    "        \"\"\"\n",
    "         \n",
    "        print (51 * '=')\n",
    "        print (24 * '#' + 'MPF' + 24 * '#')\n",
    "        print (51 * '=')\n",
    "        print (str(datetime.now()))\n",
    "        print ('Input size: {0}'.format(self.n))\n",
    "        print ('Learning temperature: {0}'.format(self.temperature))\n",
    "        print ('Learning rate: {0}'.format(lr))\n",
    "        \n",
    "        cost = np.mean(np.exp(1/self.temperature * (-(self.x - 0.5) * np.dot(self.x, self.W)) \\\n",
    "                              - 0.25 * np.diag(self.W)))\n",
    "               \n",
    "        D = np.dot((self.x - 0.5).T, self.x)\n",
    "        Wgrad = -cost * (D - 0.5 * np.diag(D) + 0.25 * np.eye(self.n))\n",
    "        \n",
    "#         Tcost = T.mean(T.exp(1/self.temperature * (-(self.x - 0.5) * T.dot(self.x, self.W)) \\\n",
    "#                               - 0.25 * T.diag(self.W)))\n",
    "#         TWgrad = T.grad(cost, self.W)\n",
    "\n",
    "        return cost, Wgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = np.array([[0],[1],[1],[1],[0],[1],[1],[1],[0],[1],[1],[1],[0],[1],[1],[1]])\n",
    "s = s.T\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
